{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection strategy: \n",
    "Use 10-fold cross validation for hyperparameter selection on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    trset = np.load(\"data/train.npy\")\n",
    "    testset = np.load(\"data/test.npy\")\n",
    "    return trset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tr_and_val(trset, n_split=10):\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_split)\n",
    "    splits = kf.split(trset)\n",
    "    return [ (trset[s[0]], trset[s[1]]) for s in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(trset, batch_size=64, shuffle=True, split_first=False,\n",
    "                                       dtype=torch.float, label_dtype=torch.long, \n",
    "                                      device=torch.device(\"cpu\")):\n",
    "    \"\"\" given a numpy array, return a DataLoader with spec'd batch size\"\"\"\n",
    "    \n",
    "    trset = torch.tensor(trset).to(device=device, dtype=dtype)\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    if split_first:\n",
    "        x, y = trset[:, 1:], trset[:, 0]\n",
    "        y = y.to(label_dtype)\n",
    "        ds = TensorDataset(x,y)\n",
    "    else:\n",
    "        ds = TensorDataset(trset)\n",
    "    \n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, optimizer, loss_function, accuracy_function,\n",
    "                  train_dl, val_dl, epochs,\n",
    "                  early_stopping=True,\n",
    "                  stop_with='acc',\n",
    "                 no_improvement_epochs=5, verbose=False):\n",
    "    \"\"\"Fit model to training set.\n",
    "        Returns: final loss function on train and val sets.\n",
    "        Early stopping based on val loss.\n",
    "        no_improvement_epochs: number of epochs of no improvement in the val loss which will trigger \n",
    "        early stopping. If -1, no early stopping. \"\"\"\n",
    "    \n",
    "    if stop_with not in ['acc', 'loss']:\n",
    "        raise ValueError\n",
    "    if no_improvement_epochs < 1:\n",
    "        raise ValueError\n",
    "    \n",
    "    def compute_loss(xb, yb):\n",
    "        out = model(xb)\n",
    "        return loss_function(out, yb)\n",
    "    \n",
    "    def compute_acc(xb,yb):\n",
    "        out = model(xb)\n",
    "        return accuracy_function(out, yb)\n",
    "    \n",
    "    def mean_stat(dl, which='loss'):\n",
    "        metric = compute_loss if which=='loss' else compute_acc\n",
    "        return (sum(metric(xb,yb) for xb, yb in dl) / len(dl)).item()\n",
    "        \n",
    "    #track losses at each epoch\n",
    "    val_losses = []\n",
    "    val_accuracy = []\n",
    "    tr_losses = []\n",
    "    tr_accuracy = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            #loss between model prediction and correct labels\n",
    "            loss = compute_loss(xb,yb)\n",
    "            #backprop into the model\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            #update params\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            #at each epoch, check losses and accuracy\n",
    "            tr_loss = mean_stat(train_dl, which='loss')\n",
    "            tr_acc = mean_stat(train_dl, which='acc')\n",
    "            tr_losses.append(tr_loss)\n",
    "            tr_accuracy.append(tr_acc)\n",
    "            \n",
    "            val_loss = mean_stat(val_dl, which='loss')\n",
    "            val_acc = mean_stat(val_dl, which='acc')\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracy.append(val_acc)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Finished epoch %d\"% epoch)\n",
    "                print(\"tr loss=\", tr_loss)\n",
    "                print(\"tr acc=\", tr_acc)\n",
    "                print(\"val loss=\", val_loss)\n",
    "                print(\"val acc=\", val_acc)\n",
    "\n",
    "        #check if early stopping is desirable\n",
    "        if early_stopping and (epoch >= no_improvement_epochs):\n",
    "            stop_metric = val_losses if stop_with == 'loss' else val_accuracy\n",
    "            recent_val = stop_metric[-(no_improvement_epochs+1):]\n",
    "            sgn = 1 if stop_with =='loss' else -1\n",
    "            if ((sgn*np.diff(recent_val))>=0).all():\n",
    "                print(\"No improvement in val for %d epochs, halting\" % no_improvement_epochs)\n",
    "                break\n",
    "    return tr_losses, tr_accuracy, val_losses, val_accuracy\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cross_val(tr_set_numpy, val_function, \n",
    "                         tr_bs = 64,\n",
    "                         val_bs = 128,n_split=10, n_train=None,\n",
    "                 device=torch.device(\"cpu\")):\n",
    "    \"\"\" Average cost when training a particular model, with a particular set of \n",
    "    hyperparams, on different train/ val splits\n",
    "        dl: dataloader containing all training data. \n",
    "        n_split: number of folds to use to split up the dataset\n",
    "        n_train: how many of these to train on and subsequently average over\n",
    "        \n",
    "        trset: numpy array holding mnist training data\"\"\"\n",
    "    if n_train is None:\n",
    "        n_train = n_split\n",
    "    if n_train > n_split:\n",
    "        raise ValueError\n",
    "    \n",
    "    tr_and_val_splits = get_tr_and_val(tr_set_numpy, n_split=n_split)\n",
    "\n",
    "    costs = []\n",
    "    for ii in range(n_train):\n",
    "        t,v = tr_and_val_splits[ii]\n",
    "        train_dl = get_dataloader(t, batch_size=tr_bs,\n",
    "                                  shuffle=True,split_first=True,label_dtype=torch.long, \n",
    "                                     device=device)\n",
    "        val_dl = get_dataloader(t, batch_size=val_bs,\n",
    "                                  shuffle=True,split_first=True,label_dtype=torch.long, \n",
    "                                     device=device)\n",
    "        \n",
    "        cost = val_function(train_dl, val_dl)\n",
    "        print(\"Metric on %dth fold:\"%ii, cost)\n",
    "        costs.append(cost)\n",
    "    return np.mean(costs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    \"\"\"a lambda-layer\"\"\"\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.func(x)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    \"\"\"Preprocess mnist data in a format amenable to conv layers\"\"\"\n",
    "    # reshape to 2d and add a (dummy) channel\n",
    "    x = x.view(-1,1, 28,28)\n",
    "    # cast to torch.float32, default for conv layers\n",
    "    return x.to(torch.float)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv model for digit recognition\n",
    "Inputs are (N, 784) real tensors.\n",
    "First, they're reshaped to 2d images with 1 channel. Then conv+relu, maxpool, conv+relu, global average pooling, and a final linear layer to produce the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convmodel():\n",
    "    return nn.Sequential( Lambda(preprocess), \n",
    "                            nn.Conv2d(1,32,3,padding=1), \n",
    "                             nn.ReLU(),\n",
    "                             nn.MaxPool2d(2),\n",
    "                            nn.Conv2d(32, 32,3,padding=1),\n",
    "                           nn.ReLU(),\n",
    "                            nn.AvgPool2d(14), \n",
    "                            Lambda(torch.squeeze),\n",
    "                            nn.Linear(32,10)\n",
    "                         )\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    \"\"\" The cross-entropy loss between model's logits and integer \n",
    "    labels 0....9.\n",
    "        logits = (N, 10) tensor of logit values. The model probs for each class are defined as the softmax\n",
    "        of the logits.\n",
    "        labels = (N,) tensor of integer labels.\n",
    "        \n",
    "        returns: scalar loss tensor\"\"\"\n",
    "\n",
    "    return nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "def acc_fn(logits, labels):\n",
    "    \"\"\"Accuracy of model predictions (defined as most likely values)\"\"\"\n",
    "    max_logits, indices = torch.max(logits, 1)\n",
    "    return (labels.to(indices.dtype)==indices).to(torch.float).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "dev=\"cuda:0\"\n",
    "device = torch.device(dev) if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dl = get_dataloader(tv[0][0], batch_size=64, shuffle=True, split_first=True, label_dtype=torch.long, \n",
    "                                     device=dev)\n",
    "val_dl = get_dataloader(tv[0][1], batch_size=256, shuffle=False, split_first=True, label_dtype=torch.long,\n",
    "                                       device=dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def val_function(train_dl, val_dl):\n",
    "    \"\"\"Defines a metric for training quality based on val performance\"\"\"\n",
    "    convmodel = get_convmodel()\n",
    "    convmodel.to(device)\n",
    "    optimizer = torch.optim.Adam(convmodel.parameters(),lr=lr)\n",
    "    tr_loss, tr_acc, val_loss, val_acc = fit_model(convmodel, optimizer, loss_fn, acc_fn,\n",
    "                                                    train_dl, val_dl, epochs,\n",
    "                                                  early_stopping=True,\n",
    "                                                  stop_with='acc',\n",
    "                                                    no_improvement_epochs=5)    \n",
    "    del convmodel\n",
    "    del optimizer\n",
    "    \n",
    "    return val_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_function(model, optimizer, epochs, \n",
    "                         early_stopping=True, stop_with = 'acc', \n",
    "                         no_improvement_epochs=5):\n",
    "    def val_function(train_dl, val_dl):\n",
    "    \n",
    "        tr_loss, tr_acc, val_loss, val_acc = fit_model(model, optimizer, loss_fn, acc_fn,\n",
    "                                                        train_dl, val_dl, epochs,\n",
    "                                                          early_stopping=early_stopping,\n",
    "                                                          stop_with=stop_with,\n",
    "                                                            no_improvement_epochs=no_improvement_epochs)    \n",
    "        \n",
    "        return val_acc[-1]\n",
    "    return val_function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(modelgen, tr_numpy, hyperparams, epochs,\n",
    "                       n_split=10, n_train=None,device=torch.device(\"cpu\"),\n",
    "                          early_stopping=True, stop_with='acc',\n",
    "                          no_improvement_epochs=5):\n",
    "    nh = len(hyperparams['lr'])\n",
    "    scores = []\n",
    "    for ii in range(nh):\n",
    "        lr = hyperparams['lr'][ii]\n",
    "        batch_size = hyperparams['batch_size'][ii]\n",
    "        model = modelgen().to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "        \n",
    "        val_fn = make_val_function(model, optimizer, epochs, early_stopping=early_stopping,\n",
    "                                            stop_with=stop_with,no_improvement_epochs=no_improvement_epochs)\n",
    "        score=do_cross_val(tr_numpy, val_fn, \n",
    "                         tr_bs=batch_size,\n",
    "                         val_bs=batch_size,n_split=n_split, n_train=n_train,\n",
    "                 device=device)\n",
    "        scores.append(score)\n",
    "    \n",
    "        print(\"score = \", score)\n",
    "        del model\n",
    "        del optimizer\n",
    "    return scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh=20\n",
    "lr = 10**np.random.uniform(-4,0,nh)\n",
    "weight_decay=10**np.random.uniform(-6,-1,nh)\n",
    "batch_size=[64] * nh\n",
    "hyperparams = dict(lr=lr,batch_size=batch_size)\n",
    "epochs=40\n",
    "n_split=5\n",
    "n_train=3\n",
    "early_stopping=True\n",
    "stop_with = 'acc'\n",
    "no_improvement_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = run_cv(get_convmodel, tr, hyperparams, epochs,\n",
    "                   n_split=n_split, \n",
    "                   n_train=n_train, device=device,early_stopping=early_stopping,\n",
    "                   stop_with=stop_with, no_improvement_epochs=no_improvement_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
