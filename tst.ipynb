{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection strategy: \n",
    "Use 10-fold cross validation for hyperparameter selection on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    trset = np.load(\"data/train.npy\")\n",
    "    testset = np.load(\"data/test.npy\")\n",
    "    return trset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "splits = kf.split(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tr_and_val(trset, n_split=10):\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_split)\n",
    "    splits = kf.split(trset)\n",
    "    return [ (trset[s[0]], trset[s[1]]) for s in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = get_tr_and_val(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(trset, batch_size=64, shuffle=True, split_first=False):\n",
    "    trset = torch.tensor(trset)\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    if split_first:\n",
    "        x, y = trset[:, 1:], trset[:, 0]\n",
    "        ds = TensorDataset(x,y)\n",
    "    else:\n",
    "        ds = TensorDataset(trset)\n",
    "    \n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = get_dataloader(tv[0][0],split_first=True,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cross_val(trset, fit_function, model, hyperparams,\n",
    "                         val_bs = 128,n_split=10, n_train=10):\n",
    "    \"\"\" Average cost when training a particular model, with a particular set of \n",
    "    hyperparams, on different train/ val splits\n",
    "        n_split: number of folds to use to split up the dataset\n",
    "        n_train: how many of these to train on and subsequently average over\n",
    "        \n",
    "        trset: numpy array holding mnist training data\"\"\"\n",
    "    \n",
    "    tr_and_val_splits = get_tr_and_val(trset, n_split=n_split)\n",
    "    costs = []\n",
    "    for ii in range(n_train):\n",
    "        t,v = tr_and_val_splits[ii]\n",
    "        train_dl = get_dataloader(t, batch_size=hyperparams['batch_size'],\n",
    "                                  shuffle=True,split_first=True)\n",
    "        val_dl = get_dataloader(t, batch_size=val_bs)\n",
    "        \n",
    "        cost = fit_function(model, train_dl, val_dl, hyperparams)\n",
    "        costs.append(cost)\n",
    "    return np.mean(costs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    \"\"\"a lambda-layer\"\"\"\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.func(x)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    \"\"\"Preprocess mnist data in a format amenable to conv layers\"\"\"\n",
    "    # reshape to 2d and add a (dummy) channel\n",
    "    x = x.view(-1,1, 28,28)\n",
    "    # cast to torch.float32, default for conv layers\n",
    "    return x.to(torch.float)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv model for digit recognition\n",
    "Inputs are (N, 784) real tensors.\n",
    "First, they're reshaped to 2d images with 1 channel. Then conv+relu, maxpool, conv+relu, global average pooling, and a final linear layer to produce the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "convmodel = nn.Sequential( Lambda(preprocess), \n",
    "                            nn.Conv2d(1,32,3,padding=1), \n",
    "                             nn.ReLU(),\n",
    "                             nn.MaxPool2d(2),\n",
    "                            nn.Conv2d(32, 32,3,padding=1),\n",
    "                           nn.ReLU(),\n",
    "                            nn.AvgPool2d(14), \n",
    "                            Lambda(torch.squeeze),\n",
    "                            nn.Linear(32,10)\n",
    "                         )\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    \"\"\" The cross-entropy loss between model's logits and integer \n",
    "    labels 0....9.\n",
    "        logits = (N, 10) tensor of logit values. The model probs for each class are defined as the softmax\n",
    "        of the logits.\n",
    "        labels = (N,) tensor of integer labels.\n",
    "        \n",
    "        returns: scalar loss tensor\"\"\"\n",
    "    return nn.function.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
